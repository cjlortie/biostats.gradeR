---
title:
author: "cjlortie"
date: "Oct 2016"
output:
  html_document:
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
---
##BIOL5081: Biostatistics GradeR   

![](./can.do.png)

The goal of the [graduate-level course](https://cjlortie.github.io/r.stats/) was statistical thinking through R and good data science. Data frames, EDA, and effective graphics can enhance our capacity to use statistical thinking to solve problems and examine evidence. Real data were used to both practice, time-bounded code (hackathon model), and follow-up revise workflow and analyses.

The purpose of this workflow is to explore whether the philosophy effectively translated into learning and evaluation. The evidence examined were the test scores from each iteration.

<b>Meta-data for scoring dimensions</b>
A total of 25% so 5 questions each worth 5 points. Likert Scale of 1 to 5 with 1 being low and 5 being awesome. The book ['The Art of Data Science'](https://leanpub.com/artofdatascience) by Peng and Metsui was extremely helpful. In particular, the EDA checklist provided in chapter 4.  

+ Can I understand what was done?
+ Can I repeat what was done?
+ Does the data viz effectively and thoroughly examine and show patterns/relations?
+ Is the EDA a clear and appropriate examination the evidence and demonstrates statistical thinking?
+ Is the final graphic and statistical test appropriate (tidy, polished, and meaningful) and suitable for publication?

###Time-bounded hackathon
```{r, test 1}
library(dplyr)
library(ggplot2)
library(tibble)
library(knitr)
library(fitdistrplus)

#read data
metrics <-read.csv('data/BIOL5081.2016.grades.csv')
#as_tibble(metrics)
hackathon <- metrics %>% filter(metric == "test1")

#table so people can see their scores
kable(hackathon)

#data viz for pattern analysis and exploration of potential stats
ggplot(hackathon, aes(dataset, total)) + 
  geom_boxplot()

ggplot(hackathon, aes(total, colour = dataset)) +
         geom_freqpoly(binwidth = 1.5)

#EDA on distribution
shapiro.test(hackathon$total) 
qqnorm(hackathon$total)
descdist(hackathon$total, boot = 1000)

summary(hackathon)
#mean is 80% and median 84%. Nice!

#variation
m <- mean(hackathon$total)
v<- sd(hackathon$total)
cv <- v/m #coefficient of variation
cv #very small CV for the total scores on code. that is good news.

#frequencies of real dataset use
ggplot(hackathon, aes(dataset)) + geom_bar(stat = "count") + coord_polar(theta="x", start = 0, direction = 1) +xlab("") + ylab("")

ggplot(hackathon, aes(dataset)) + geom_bar(stat = "count") + coord_polar(theta="y", start = 0, direction = 1) +xlab("") + ylab("")

#these plots are tough. I do not want a barchart though.

popular.datasets <- hackathon %>% group_by(dataset) %>% tally() 
#ggplot(popular.datasets, aes(dataset, sort(n))) + 
  #geom_density() + coord_flip() 

#looks like running was most popular
c1<-chisq.test(popular.datasets$dataset, popular.datasets$n)
c1 #no difference but too few observations, so need to simulate

c2<-chisq.test(popular.datasets$dataset, popular.datasets$n, simulate.p.value = TRUE, B = 10000)
c2 #no difference in frequency even with 10,000 replicates of this test

#indication that the dataset related to total score
m1 <- lm(total~dataset, data = hackathon) #or just do aov? about the same really.
summary(m1)
anova(m1, test="Chisq") #no indication that the dataset significantly influenced scores
shapiro.test(m1$residuals)

#however, sample sizes very uneven
m2 <- kruskal.test(total~dataset, data = hackathon)
m2 #still no difference

#Interactive viz using plot.ly or highchart

#final model as lm?

#Note - discovery, these data are messy. Each column is not independent, i.e. the 5 questions scored on the Likert Scale. Need to be tidied up to examine whether a specific dimension was low and thus whether instruction was inadequate or could have been improved.

```



